//app
tapas {
	spark {
		trainer = "spark://localhost:8001"
		predictor = "spark://localhost:8002"
		analyzer = "spark://localhost:8002"		
	}  
}


//system
akka {
  loggers = ["akka.event.Logging$DefaultLogger"]
  loglevel = "INFO"
  
  stream {
    # Default materializer settings
    materializer {

      # Initial size of buffers used in stream elements
      initial-input-buffer-size = 4
      # Maximum size of buffers used in stream elements
      max-input-buffer-size = 16

      # Fully qualified config path which holds the dispatcher configuration
      # to be used by ActorMaterializer when creating Actors.
      # When this value is left empty, the default-dispatcher will be used.
      dispatcher = ""

      blocking-io-dispatcher = "akka.stream.default-blocking-io-dispatcher"

      # Cleanup leaked publishers and subscribers when they are not used within a given
      # deadline
      subscription-timeout {
        # when the subscription timeout is reached one of the following strategies on
        # the "stale" publisher:
        # cancel - cancel it (via `onError` or subscribing to the publisher and
        #          `cancel()`ing the subscription right away
        # warn   - log a warning statement about the stale element (then drop the
        #          reference to it)
        # noop   - do nothing (not recommended)
        mode = cancel

        # time after which a subscriber / publisher is considered stale and eligible
        # for cancelation (see `akka.stream.subscription-timeout.mode`)
        timeout = 5s
      }

      # Enable additional troubleshooting logging at DEBUG log level
      debug-logging = off

      # Maximum number of elements emitted in batch if downstream signals large demand
      output-burst-limit = 1000

      # Enable automatic fusing of all graphs that are run. For short-lived streams
      # this may cause an initial runtime overhead, but most of the time fusing is
      # desirable since it reduces the number of Actors that are created.
      # Deprecated, since Akka 2.5.0, setting does not have any effect.
      auto-fusing = on

      # Those stream elements which have explicit buffers (like mapAsync, mapAsyncUnordered,
      # buffer, flatMapMerge, Source.actorRef, Source.queue, etc.) will preallocate a fixed
      # buffer upon stream materialization if the requested buffer size is less than this
      # configuration parameter. The default is very high because failing early is better
      # than failing under load.
      #
      # Buffers sized larger than this will dynamically grow/shrink and consume more memory
      # per element than the fixed size buffers.
      max-fixed-buffer-size = 1000000000

      # Maximum number of sync messages that actor can process for stream to substream communication.
      # Parameter allows to interrupt synchronous processing to get upstream/downstream messages.
      # Allows to accelerate message processing that happening within same actor but keep system responsive.
      sync-processing-limit = 1000

      debug {
        # Enables the fuzzing mode which increases the chance of race conditions
        # by aggressively reordering events and making certain operations more
        # concurrent than usual.
        # This setting is for testing purposes, NEVER enable this in a production
        # environment!
        # To get the best results, try combining this setting with a throughput
        # of 1 on the corresponding dispatchers.
        fuzzing-mode = off
      }

      io.tcp {
        # The outgoing bytes are accumulated in a buffer while waiting for acknoledgment
        # of pending write. This improves throughput for small messages (frames) without
        # sacrificing latency. While waiting for the ack the stage will eagerly pull
        # from upstream until the buffer exceeds this size. That means that the buffer may hold
        # slightly more bytes than this limit (at most one element more). It can be set to 0
        # to disable the usage of the buffer.
        write-buffer-size = 16 KiB
      }

      //#stream-ref
      # configure defaults for SourceRef and SinkRef
      stream-ref {
        # Buffer of a SinkRef that is used to batch Request elements from the other side of the stream ref
        #
        # The buffer will be attempted to be filled eagerly even while the local stage did not request elements,
        # because the delay of requesting over network boundaries is much higher.
        buffer-capacity = 32

        # Demand is signalled by sending a cumulative demand message ("requesting messages until the n-th sequence number)
        # Using a cumulative demand model allows us to re-deliver the demand message in case of message loss (which should
        # be very rare in any case, yet possible -- mostly under connection break-down and re-establishment).
        #
        # The semantics of handling and updating the demand however are in-line with what Reactive Streams dictates.
        #
        # In normal operation, demand is signalled in response to arriving elements, however if no new elements arrive
        # within `demand-redelivery-interval` a re-delivery of the demand will be triggered, assuming that it may have gotten lost.
        demand-redelivery-interval = 1 second

        # Subscription timeout, during which the "remote side" MUST subscribe (materialize) the handed out stream ref.
        # This timeout does not have to be very low in normal situations, since the remote side may also need to
        # prepare things before it is ready to materialize the reference. However the timeout is needed to avoid leaking
        # in-active streams which are never subscribed to.
        subscription-timeout = 30 seconds

        # In order to guard the receiving end of a stream ref from never terminating (since awaiting a Completion or Failed
        # message) after / before a Terminated is seen, a special timeout is applied once Terminated is received by it.
        # This allows us to terminate stream refs that have been targeted to other nodes which are Downed, and as such the
        # other side of the stream ref would never send the "final" terminal message.
        #
        # The timeout specifically means the time between the Terminated signal being received and when the local SourceRef
        # determines to fail itself, assuming there was message loss or a complete partition of the completion signal.
        final-termination-signal-deadline = 2 seconds
      }
      //#stream-ref
    }
  }

  # configure overrides to ssl-configuration here (to be used by akka-streams, and akka-http â€“ i.e. when serving https connections)
  ssl-config {
    protocol = "TLSv1.2"
  }

  kafka {
  	producer {
	  # Tuning parameter of how many sends that can run in parallel.
	  parallelism = 100
	
	  # Duration to wait for `KafkaProducer.close` to finish.
	  close-timeout = 60s
	
	  # Fully qualified config path which holds the dispatcher configuration
	  # to be used by the producer stages. Some blocking may occur.
	  # When this value is empty, the dispatcher configured for the stream
	  # will be used.
	  use-dispatcher = "akka.kafka.default-dispatcher"
	
	  # The time interval to commit a transaction when using the `Transactional.sink` or `Transactional.flow`
	  # for exactly-once-semantics processing.
	  eos-commit-interval = 100ms
	
	  # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
	  # can be defined in this configuration section.
	  kafka-clients {
	    bootstrap.servers = "localhost:9092"
	  }  	
  	}
  	
  	consumer {
	  # Tuning property of scheduled polls.
	  # Controls the interval from one scheduled poll to the next.
	  poll-interval = 50ms
	
	  # Tuning property of the `KafkaConsumer.poll` parameter.
	  # Note that non-zero value means that the thread that
	  # is executing the stage will be blocked. See also the `wakup-timeout` setting below.
	  poll-timeout = 50ms
	
	  # The stage will delay stopping the internal actor to allow processing of
	  # messages already in the stream (required for successful committing).
	  # This can be set to 0 for streams using `DrainingControl`.
	  stop-timeout = 30s
	
	  # Duration to wait for `KafkaConsumer.close` to finish.
	  close-timeout = 20s
	
	  # If offset commit requests are not completed within this timeout
	  # the returned Future is completed `CommitTimeoutException`.
	  # The `Transactional.source` waits this ammount of time for the producer to mark messages as not
	  # being in flight anymore as well as waiting for messages to drain, when rebalance is triggered.
	  commit-timeout = 15s
	
	  # If commits take longer than this time a warning is logged
	  commit-time-warning = 1s
	
	  # Not relevant for Kafka after version 2.1.0.
	  # If set to a finite duration, the consumer will re-send the last committed offsets periodically
	  # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
	  commit-refresh-interval = infinite
	
	  # Fully qualified config path which holds the dispatcher configuration
	  # to be used by the KafkaConsumerActor. Some blocking may occur.
	  use-dispatcher = "akka.kafka.default-dispatcher"

	  kafka-clients {
	    # auto-commit disabled by default
	    # Setting enable.auto.commit means that offsets are committed automatically 
	    #  with a frequency controlled by the config auto.commit.interval.ms.
	    enable.auto.commit = true
	
	    bootstrap.servers = "localhost:9092"
	    group.id = "group1"
	
	    auto.offset.reset = "earliest"
	  }
	
	  # Time to wait for pending requests when a partition is closed
	  wait-close-partition = 500ms
	
	  # Limits the query to Kafka for a topic's position
	  position-timeout = 5s
	
	  # When using `AssignmentOffsetsForTimes` subscriptions: timeout for the
	  # call to Kafka's API
	  offset-for-times-timeout = 5s
	
	  # Timeout for akka.kafka.Metadata requests
	  # This value is used instead of Kafka's default from `default.api.timeout.ms`
	  # which is 1 minute.
	  metadata-request-timeout = 5s
	
	  # Interval for checking that transaction was completed before closing the consumer.
	  # Used in the transactional flow for exactly-once-semantics processing.
	  eos-draining-check-interval = 30ms
	
	  # Issue warnings when a call to a partition assignment handler method takes
	  # longer than this.
	  partition-handler-warning = 5s
	
	  # Settings for checking the connection to the Kafka broker. Connection checking uses `listTopics` requests with the timeout
	  # configured by `consumer.metadata-request-timeout`
	  connection-checker {
	
	    #Flag to turn on connection checker
	    enable = false
	
	    # Amount of attempts to be performed after a first connection failure occurs
	    # Required, non-negative integer
	    max-retries = 3
	
	    # Interval for the connection check. Used as the base for exponential retry.
	    check-interval = 15s
	
	    # Check interval multiplier for backoff interval
	    # Required, positive number
	    backoff-factor = 2.0
	  }  	
	}
	
	committer {
	  # Maximum number of messages in a single commit batch
	  max-batch = 1000
	
	  # Maximum interval between commits
	  max-interval = 10s
	
	  # Parallelsim for async committing
	  parallelism = 100
	
	  # API may change.
	  # Delivery of commits to the internal actor
	  # WaitForAck: Expect replies for commits, and backpressure the stream if replies do not arrive.
	  # SendAndForget: Send off commits to the internal actor without expecting replies (experimental feature since 1.1)
	  delivery = WaitForAck	
	}
	
	default-dispatcher {
	  	type = "Dispatcher"
	  	executor = "thread-pool-executor"
	
	  	thread-pool-executor {
	    		fixed-pool-size = 16
		}  	
	}
  }  
}  
